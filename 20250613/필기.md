Mask-RCNN 도전해보기!
객체가 있을 만한 영역을 탐지하고 탐지한 영역 내 범주를 예측한 뒤, 탐지한 영역 내 픽셀 별로 범주를 예측하는 방법론
detection task보다는 instance segmentation task에서 주로 사용
Semantic segmentation: 같은 클래스를 같은 영역으로 분류
Instance segmenation: 같은 클래스라도 다른 인스턴스로 분류
핵심 아이디어
1. Faster R-CNN + mask branch
2. 각 RoI의 feature extraction에 RoIPool 대신 RoIAlign을 사용한다.
3. mask prediction과 class prediction을 분리(decoupling)했다. 
RolPool
Faster R-CNN은 object detection을 위한 모델이였기 때문에 RoIPool과정에서 정확한 위치정보를 담는 것은 별로 중요하지않았음
RolPool
RoIPool방식은 Quantization해주고 나서 Pooling을 해주는 것
RoI가 소수점 좌표를 가지고있으면 각 좌표를 반올림하여 Pooling
RolPool
input image의 원본 위치정보가 왜곡
classification을 하는데 문제가 안되지만 인스턴스 세그멘테이션처럼 픽셀별로 detection하는 경우는 문제가 생김
RoIAlign
RoI 영역을 pooling layer의 크기에 맞추어 등분한 후 샘플 포인트들을 잡음
RoIAlign
각 그리드에 sample point들을 잡음
그림을 봤을 때, 한 그리드에 4개의 샘플포인트, 총 16개의 샘플포인트가 있음
RoIAlign
sample point 하나를 기준으로 가까운 그리드 셀 4개에 대해서 bilinear interpolation(양선형 보간법) 계산을 해서 Feature Map을 계산
위의 과정을 모든 sample point에 대해 진행
하나의 영역 안에 4개의 값이 생기게 되는데 max 또는 average pooling을 사용해 2x2의 output을 얻어낼 수 있음

Mask R-CNN 
Mask R-CNN은 분류, 회귀, masking을 병렬로 수행

Mask R-CNN은 백본 (RESNET, FPN 등 다양한 백본을 연결 해서 사용할수 있도록 해줌)




Fully connected to Conv Layer
Fully connected to Conv Layer
패딩 없이 컨볼루션과 맥스 풀링을 반복하면 feature map 크기가 빠르게 줄어듬
Fully connected to Conv Layer
패딩을 사용하면 feature map 크기 감소가 완만해지고, 출력 크기가 더 크게 유지됨
YOLO grid
이미지를 격자로 분할하여 각 셀이 객체 정보 예측
셀별 바운딩 박스와 클래스 확률 출력
전체 예측 결과의 행렬화
YOLO 학습
학습 시 center의 위치에 해당하는 cell에 결과 입력
Center의 좌표 값은 grid 좌상단 좌표의 offset으로 입력
YOLO 학습
한 cell의 출력 결과가 1개일 경우, 복수의 중첩된 객체의 감지가 불가능
Anchor box를 도입하여 복수의 출력을 가능하게 함
YOLO 학습
미리 전체 이미지의 크기에 따라 object가 많은 형태의 anchor를 선정
Anchor box의 수 만큼 결과를 증가
YOLO 학습
Ground truth와 anchor의 IoU가 최대인 anchor에 대당하는 곳으로 학습 데이터를 입력
Full ConvNet
이미지부터 분류 및 bounding box regression까지 전체 네트워크를 CNN으로 구성하여 학습
한번의 CNN으로 추론 도출
Grid와 cell별 anchor로 분류 결과 및 bonding box 결과 출력

SxS grid on input -> Boundinf boxes +confidence -> Final detections
                  -> Class Probability map ->
 Non Max Suppression
최종 출력 텐서에 NMS를 적용하여 중복된 box들 중 높은 confidence score만 선택

Backbone 이란?
CNN Classifier에서 최종 FC 레이어를 제외한 특징 추출 부분
Object detection에서는 여러가지 backbone을 활용 가능
성능과 속도를 고려하여 적합한 네트워크를 선정
Batch Normalization 이란?
Conv -> ReLU 구조에서는 입력 분포가 한쪽으로 치우칠 수 있음
BatchNorm을 적용하면 분포가 정규화되어 학습이 안정되고 표현력이 향상됨
Batch Normalization 이란?
BN은 평균과 분산을 기준으로 정규화 후, 선형변환을 통해 표현력을 유지
이를 통해 손실 합수의 지형이 평탄해져, 더 빠르고 안정적인 학습이 가능
Mish activation function
Swish와 유사한 비선형 함수로, 𝑀𝑖𝑠ℎ(𝑥)=𝑋 ∗tanh⁡(ln⁡(1+𝑒^𝑥 ) ) 형태를 가짐
ReLU보다 부드러운 곡선으로 정보 흐름을 보존하고 성능 향상에 도움을 줌

YOLO Backbone
YOLO v1 – VGG-19 backbone을 사용
YOLO v2 – Darknet-19 사용
Vgg-19와 유사
1*1 conv를 활용하여 속도와 성능 개선
Global average pooling을 사용
YOLO v3 – Darknet-53
Resnet과 비슷한 skip connection도입
기울기 소실 문제를 완화
Layer수 증가에 따른 성능 향상
YOLO v4 – CSPdarknet-53
Cross stage partial network 도입
특징 맵을 두 갈래로 나누어 일부는 Residual Block에 통과시키고, 나머지는 우회시켜 합침
연산량을 줄이면서도 gradient의 흐름을 보존하여 학습 안정성과 정확도 항샹
Layer수 증가에 따른 성능 향상
SPP (Spatial Pyramid Pooling)
NIN(network in network)과 유사
13*13, 9*9, 5*5, 1*1 maxpooling
Stride=1과 같은 W*H 출력
Backbone 다음 레이어로 사용
SPP (Spatial Pyramid Pooling)
여러 scale의 feature를 detection에 전달
보다 다양한 크기의 감지에 유용
COCO 기준 2.7% 성능 (mAP50) 향상
YOLO Neck
FPN(Feature Pyramid Network)
Backbone의 최종 레이어 및 사이즈 변경 직전의 feature layer들을 가지고 복수의 detection 수행
레이어 별로 다른 크기 감지 (최종 레이어 : 모든 정제된 feature + 대락적 위치, 중간 레이어 : 덜 정제된 feature +  상세 위치)
상위 레이어의 feature를 업샘플링하여 중간 레이어의 feature를 보강
Path Aggregation Network
FPN의 Layer들을 다시 역방향으로 더하거나, concatenate하여 최종 head로 전달
정확한 위치 기반의 예측이 가능해져, 소형 객체 검출 성능 향상
DIoU (distance-IoU)
IoU에 두 박스 중심 간 거리(d)를 추가로 고려하여, 박스 간의 겹침 정도 + 위치 관계를 동시에 반영
박스 중심 간 거리(c)가 작고, 최소 외접 박스의 대각선 길이가 작을수록 더 높은 DIoU를 가짐
Box Loss
작은 객체에 더 집중하기 위해 객체 크기에 따른 가중치 조정
최종 box loss는 CIoU 손실에 가중치를 곱해, 객체가 있는 위치만 합산
Class Loss
객체가 존재하는 위치 (Obj=1)에 대해서만 클래스 예측과 정답 간 Binary Cross Entropy를 계산
Binary Cross Entropy: 0이나 1이냐 이진분류에 사용
멀티클래스 분류를 위한 one vs rest 방식으로 모든 클래스에 대해 BCE 손실을 합산
Confidence Loss
객체가 있는 셀 (Obj=1)에 대해 예측 확률이 높도록 -log(p)값을 최소화
객체가 없는 셀 (Obj=0)에 대해 잘못된 예측을 억제하기 위해 -log(1-p)를 최소화
객체 존재 여부에 따라 서로 다른 손실 항을 적용하여 예측 신뢰도를 조절
Total Loss
L=Lbox + Lclass + Lobj conf + Lnoobj conf
YOLO 성능
빠른 실행 속도
적절한 성능 대비 높은 수행 속도
Single GPU에서 실시간 동영상 지원, 일반 개발자들이 사용하기에 적절

모바일 대응
OpenCV 등 지원
Yolo tiny
EfficientDet
BiFPN 구조 사용
Bidirectional Feature Pyramid Network를 사용해 상하 방향 feature을 반복적으로 융합하고, 학습 가능한 가중치로 중요 feature를 선택
적은 연산량으로도 높은 정확도를 달성하는 경량, 고성능 객체 검출 모델
NAS-optimized backbone(EfficientNet) + BiFPN
EfficientNet을 백본으로 사용하고, 다중 해상도 특징을 BiFPN에서 반복적으로 융합
각 층의 출력은 Class/Box prediction head로 연결되어 정밀하고 효율적인 객체 검출 수행

head: 최종 결과물을 수행 두개이상이면 멀티헤드

YOLO- You Only Look Once
이미지에서 바운딩 박스를 찾는 방법을 사용하지 않음, 그리드(격자)로 만들어진 부분들에서 계산을 해서 바운딕 박스와 클래스피케이션을 동시에 수행
한번만 이미지를 바으면 거기서 객체 인식을 완료하여 YOLO라고 불림

fast rcnn : 2000개의 바운딩박스를 한번에 피쳐로 넣어서 오래걸림

YOLO- You Only Look Once
obejct detection 문제를 classification이 아닌 regression에 해당하는 문제로 치환
통계학에서 회귀란, 여러 개의 독립변수와 한 개의 종속변수 간의 상관관계를 모델링하는 기법 (즉 f(X) 와 같은 함수 형태로 변환되어 빠르게 계산이 가능 할수 있음)

YOLO- You Only Look Once
YOLO는 1-stage detector로, 448 x 448 x 3 size의 이미지를 input으로 받아 7 x 7 x 30 size의 tensor를 output으로 내놓는 CNN을 중심으로 하는 모델
단일 conv net을 돌려 bbox들과 그에 대한 class probability를 예측 (regression),
NMS(Non-Maximum Suppression)를 통해 최종 detection을 수행
Non-maximum Suppression 이란?
비최대 억제 알고리즘으로 생각하면 된다. 말 그대로, 최대가 아닌 박스들(=Bounding Box)을 삭제하는 알고리즘
Object Detection Task에서 객체에 대한 최종 Bounding Box를 결정지을 때 사용
Confidence-Score?
네트워크가 정답을 도출해냈을 때, 그 정답에 대해 n%의 확신도를 갖는다는 의미

IoU?
Intersection Over Union의 약자로 교집합/합집합을 의미
Prediction할 이미지를 Network에 Forward 
출력 가능한 모든 박스를 함 (NMS 이전이므로, 수십~수백개의 박스 정보가 나옴)
해당 박스 정보들을 Confidence Score 순으로 정렬
박스 정보들의 Confidence Score와, 각 박스 간의 IoU를 바탕으로 하여 NMS를 적용
NMS 적용 후에는, 알맞는 박스라고 여겨지는 값들만 출력
input image 위에 가로 및 세로로 각각 S개의 격자를 그림
각 격자는 1) bbox와 2) 각 bbox에 대한 신뢰도 점수를 예측
신뢰도 점수는 1) bbox가 객체를 포함하는지에 대해 신뢰성이 있는지와 2) bbox가 얼마나 정확한지를 반영
Confidence 값이 객체의 존재 여부와 바운딩 박스 정확도를 동시에 평가하는 지표가 될 수 있음 
Bbox + confidence & class probability
 각 grid cell은 bbox와 confidence score을 예측합니다. 여기에 더해서 class probability까지 예측
bbox는 중심인 (x, y) 좌표, 높이와 너비 (w, h) 정보, 그리고 IoU 총 5개의 변수에 대한 예측을 진행
더불어 '객체가 있다고 가정할 때, 특정 객체 (class)일 확률'이라는 조건부 확률을 예측
YOLO는 20개의 class를 지원하기 때문에 C1 ~ C20까지 총 20개의 확률 정보를 담고 있음
Test를 할 때에는, 특정 클래스에 대한 신뢰도 점수를 구하기 위한 공식을 사용
결론: 강한 녀석만 살아남는다
Final detections
마지막으로 NMS를 통해 최종 detection을 결정
Network Design
24개의 convolution layer와 2개의 fully connected layer로 구성
중간중간에 max-pooling layer와 feature map의 dimension을 줄이기 위한 (= parameter 수를 줄이기 위한) 1 x 1 convolution layer가 있음
마지막 레이어 분석
output의 정보가 곧바로 input image에 대해 해석될 수 있도록 S x S (=7 x 7) size로 맞춤
 output을 각 grid cell에 대한 output으로 나눌 수 있어서 위치 정보를 효과적으로 보전 할 수 있음
Unified Detection 파트에서 각 grid cell은 B (=2)개의 bbox와 C (=20)개의 class probability를 예측함 따라서 output의 depth를 (5 x B + C)인 30으로 맞춰준 것
YOLO- You Only Look Once
YOLO 이미지는 448X448 을 쓰기 때문에 일반적인 CONV Layer 계산 법으로는 파라미터를 계산할 수 없음



---

### **Convolution layer의 output tensor size**

* 각각 기호를 아래와 같이 정의

  * $O$: Size(width) of output image
  * $I$: Size(width) of input image
  * $K$: Size(width) of kernels used in the Conv layer
  * $N$: Number of kernels
  * $S$: Stride of the convolution operation
  * $P$: Padding size

* $O$(Size(width) of output image)는 다음과 같이 정의됨

  $$
  O = \frac{I - K + 2P}{S} + 1
  $$

* 출력 이미지의 채널 수는 커널의 갯수($N$)와 같음

---



(448 - 7 + 2P) / 2 + 1 로 계산될 수 있는데 P가 자연수이기 때문에 결과가 소수로 나옴!
이를 해결하기 위해서 제로 패딩을 양쪽에서 주는게 아니라 한쪽만 주는 방식으로 해결 (asymmetrically)
Training
Layer들에 ReLU대신 Leaky ReLU를 사용
ReLU와 유일한 차이점으로는 max(0, z)가 아닌 max(0.01z, z)라는 점
input값인 z가 음수일 경우 기울기가 0이 아닌 0.01값을 갖으며 YOLO 같은 경우에서는 ReLU 처럼 0을 만드는것 보다 약간의 기울기를 만드는게 학습 결과가 좋았다고 함
Loss function
알고리즘이 예측한 값과 실제 정답의 차이를 비교하기 위한 함수
'학습 중에 알고리즘이 얼마나 잘못 예측하는 정도'를 확인하기 위한 함수로써 최적화(Optimization)를 위해 최소화하는 것이 목적인 함수
bbox의 위치와 관련된 파트는 5를 곱하고, 객체가 없는 상황에 대한 파트는 0.5를 곱해서 더해줌으로써 중요도를 loss에 반영
5 종류의 SSE (오차제곱합)을 사용함 (오차를 최소화하여 최적화에 사용) – 일반적인 원본 참고

 PASCAL VOC의 특징
이런 형태로 표시가 되어 있기 때문에 욜로에서 읽을때 욜로 포맷 (center, center, width, height) 형태로 변환 해야함

Darknet 고찰
YOLO의 저자들은 Darknet 이라는 자체 CNN 망을 만들었음 
DarkNet의 특성은 입력받는 이미지의 해상도가 448x448로 고해상도임
 Detection이 고해상도 이미지를 종종 요구하기 때문에 이렇게 디자인을 했다고 논문에 기술하긴 함
문제는 이를 학습하는데 걸리는 처리 속도가 많이 걸림, 그리고 시간이 지나서 Darknet 자체가 좋은 CNN 망이 아니게 됨 (VGG 만 해도 훨씬 좋은 성능을 가짐
Draknet 은 C++ 및 다른 언어로 만들어진 딥러닝 프레임워크이고 (텐서플로나 파이토치 같은) 여기를 통해서 욜로를 따로 구현되는 형태로 만들어짐 (욜로 1, 2, 3 저자들이 작업 했을 당시에 가지는 특징 – 실시간 처리를 위해서 이렇게 구성함)
따라서 Darknet 을 따라가지 않고 파이썬 TF 에서 다른 백본망을 쓰는게 요새 트렌드임 (파이토치도 비슷)
VGG16를 사용할 예정
이유는 VGG가 받던 dal지 크기가 224x224 로 448x448보다 4배 작아서 (가로 세로로 딱 절반) 학습 속도가 빠르고, 정확도도 더 높음
Darknet 고찰
DarkNet은 CNN Layer 20층으로 구성되었고 Head는 CNN Layer 4층, FCN 2층으로 구성
다음의 그림에 이어지는 부분까지만 Darknet 임
나머지 부분은 헤더 부분임
![alt text](image-1.png)

데이터셋에 있는 사진(jpg), 라벨 데이터(xml)의 경로를 리스트에 담음
image_file_path_list[0] 과 xml_file_path[0] 은 2012_000001 이 됨 (똑 같이 동기화 되어야함!)
데이터셋에 있는 클래스 종류 알아내는 함수
xml파일 리스트를 받아 YOLO를 훈련시킬 데이터셋에 있는 객체 종류를 알아냄! 
PASCAL VOC 의 경우 최대 20가지가 될 수 있음
중복이 되는 경우에 문제가 될수 있으므로 파이썬의 set 자료구조를 사용함 (중복을 제거해줌)
모든 라벨링 데이터에 대해서 set 이 설정 되면 그후에 리스트로 만들어서 사용
Sort 를 해서 알파벳 순으로 정리를 해주고 리턴해주자!

욜로는 backbone -> head -> neck으로 이루어짐

Loss Function
미니배치 단위(64)로 입력 데이터가 들어오고 출력데이터가 나오기 때문에 [7,7,30] 사이즈씩 뽑아서 loss를 계산
[7,7,30]을 [49,30]으로 변경 후 [1,30] 사이즈를 가진 텐서별로 loss를 구함
인덱스 0~4는 bounding box1, 5~9는 bounding box2, 10~29는 class score이며 해당 셀에 객체의 중심이 있을 경우 예측한 bounding box 중 IoU가 높은 Bounding box와의 localization loss, confidence loss를 구하고 class score도 구함
논문에 나왔던 시그마가 어떻게 표현되는지를 보고 감을 잡아 보자 (코드가 길어서 공개 예정

LMS 에서 설정한게 잘 보이지 않아서 0.2까지 낮춰봄.. (현재 학습에 대한 성능 개선 LMS 의 개선등이 필요함)
학습 데이터셋을 늘리고 EPOCH 를 더 늘리고, LMS 도 어제 코딩한것 처럼 제대로 돌아 가도록 해야함! (이런 부분이 미니 프로젝트 주제입니다!)
혹은 OpenCV를 사용하여 실시간 카메라에 연동하여 출력을 하도록 해보는 것도 좋은 프로젝트 주제임
GPU 를 통합 학습의 경우 논문을 따라 코딩한 망 및 구현 소스들의 최적화가 필요한 상태임 – 3080RTX 를 쓰는 제 노트북에서는 메모리 문제가 계속 발생 – 결국 CPU모드로 돌림
배치수를 줄이고, GC 및 CUDA 캐시 메모리를 주기적으로 삭제해서 메모리 관리를 해야함
우리가 구현한 모델 성능은 사실 떨어지는 편이므로 YOLO7 등 예제를 적용한 빠른 응용 애플리케이션들에 적용 해도 좋음

