RCNN의 정의 
영역을 설정하고 cnns을 활용하여 물체인식을 수행하는 신경망
선택적 탐색을 통해 영역설정
선택적 탐색을 통해 임의의 BB를 설정
임의의 bb와 사전에 준비한 정답BB의 IOU를 계산
IOU가 특정 값 이상이 되도록 임의 영역즐 조절

선택적 탐색
전부를 탐색하는  완전 탐색과는 달리 특정 기준에 따라 탐색을 실시하는 것으로 여기는 상향식(bottom-up)의 탐색 방법중 하나인 계층적 그룹 알고리즘등이 사용됨

작은 크기의 초기영역을 설정
그래프 이론 객체 각네 짝을 이루는 관계를 모델링하기 위해 사용되는 수학 구조인 그래프에 대한 연구이다. 그래프는 꼭지점 교점 점으로 구성되며 이들은 선 또는 변으로 연결된다(출처 - 위키 백과)

이어져 있으면 같은 세그먼트 이어져있지 않으면 다른 세그먼트

예를들어 자동차를 인식할때 작은 영역이 자동차 안에 여러개 있어서 차를 인식하면 하나의 큰 영역으로 합친다

SVM은 기계학습의 분야 중 하나로 패턴 인식, 자료 분석 등을 위한 지도 학습 모델이며, 주로 분류와 회귀 분석을 위해 사용한다

테두리 상자 조정

이미지넷의 데이터셋을 바탕으로 CNNs(논문에서는 AlexNet)을 미리 학습
미리학습된 CNNs을 해당 작업(물체 인식)을 위해 미세조정(파인튜닝)
미세조정을 통해 조정된 SVMs과 바운딩 박스 리그레션(IOU>=0.5)를 학습시킴

R-CNN 한계점
• 합성곱 신경망의 입력을 위한 고정된 크기를 위해 warping/crop을 사용해야하며 그 과
정에서 이미지 정보 손실이 일어난다.
• 2000개의 영역마다 CNN을 적용해야 하기에 학습 시간이 오래 걸린다.
• 학습이 여러 단계로 이루어지며 이로 인해 긴 학습 시간과 대용량 저장 공간이 요구된다.
• Object Detection의 속도 자체도 느리다.

쉽게말해 오래걸리지만 디텍션에 처음 나온것

Faster RCNN

RCNN 리뷰
1.Selective Search을 통해 약 2000개의 객체가 있을만한 영역을 추천
2.2000개의 Region Proposal들의 사이지를 동일하게 맞춰야 한다


---

### 이미지 1

1. Selective Search를 통해 약 2000개의 객체가 있을만한 영역을 추천
2. 2000개의 각 Region Proposal 마다 하나씩 Feature Extractor로 입력됨
3. 2000개의 Region Proposal들의 사이즈를 통일하게 맞춰주어야 한다.
4. VGG, Resnet과 같은 Pretrained된 Feature Extractor에는 입력받는 사이즈가 고정되어 있기 때문
5. 따라서 Region Proposal을 자르거나(Crop) 찌그러뜨리는(Warp) 작업을 하고 넣어줌

---

### 이미지 2

1. Region Proposal을 기반으로 입력된 이미지에서 객체가 어떤 물체를 나타내는지 클래스 분류함
2. Feature Map을 Flatten 하고 Dense layer(FC layer)를 추가
3. Softmax layer와 SVM Classifier가 둘다 존재함
4. 먼저 Softmax layer로 객체가 어떤 클래스든 분류되는지에 대한 확률 Score를 얻음 (확률 Score로 파라미터 학습)
5. 그 후 SVM Classifier로 탐지된 객체의 클래스가 무엇인지 최종 분류

탐지된 객체의 클래스가 어떤 물체인지 분류까지 완료되면, 이제 이 객체가 어디에 있는지 Detection 하기 위해 객체 주변에 사각형의 좌표를 찾아야 함

---

### 이미지 3

최적의 좌표를 찾기 위해 Regression을 사용한다.
최적의 좌표를 찾는 과정도 Loss 함수를 이용한 Back Propagation을 진행한다.

---

### 이미지 4

1. 하나의 Region Proposal 마다 Feature Extractor에 넣어주어 Object Detection 진행
   즉, 하나의 이미지에 대해서 2000개의 Region Proposal을 진행 (대략 하나의 이미지당 47초가 소요)
   \= 학습 시간에 매우 오래 걸린다.
2. 사이즈를 고정시켜 줘야 하므로 이미지를 자르거나(crop), 찌그러뜨림(warp) 수 밖에 없다.

---

### 이미지 5

1. RCNN 모델의 단점

* 각 Region Proposal 마다 Object Detection을 별개로 수행해주어야 해서 **매우 오랜 학습시간이 소요**된다.
* 여러개의 Region Proposal들을 Pretrained된 Feature Extractor가 요구하는 \*\*고정된 사이즈로 통일시켜주어야 해서 Region Proposal을 자르거나 찌그러뜨려 이미지들을 손상(?)\*\*시키게 된다.

원본이미지 추출(Selective search)에서 추출한 것만 Feature extractor에 넣어서 피쳐맵을 추출하자!
근데 이게 안됐다.
Dense layer에 가야하는데 피쳐맵 사이즈를 고정 해줘야 된다. 근데 그게 안되는 것이다.
Flattened fc에 넣기 위한 고정 사이즈가 필요하다.
이걸 할 수 있는 별도의 레이어를 만들자!

---

SPPnet

### ✅ 이미지 1

> 첫 번째 Spatial Pyramid Pooling Layer(위 그림에서 맨 오른쪽 계층) 에서는 spatial bin의 개수가 1개이다. 즉 1개의 bin이 Image 전체를 커버한다.
> 가운데 Spatial Pyramid Pooling Layer에는 Spatial bin이 4개 있다.
> 가장 왼쪽 Spatial Pyramid Pooling 계층에는 Spatial bin이 16개 있다.
>
> 예를, 256은 5번째 Convolution Layer의 Filter 개수이자 Output Feature Map의 Channel 개수이며, 이는 SPP Layer에 들어가는 Input Feature Map의 Channel 수이다.

---

### ✅ 이미지 2

> 각 Spatial bin마다 response를 Pooling 한다. (SPPnet에서는 Max-Pooling을 사용한다)
>
> 이때, Spatial Pyramid Pooling의 결과 vector는 M × k 크기를 갖는 vector이다.
>
> M은 Spatial bin 개수의 합이다.
> k는 conv5의 Filter 개수이자, Output feature map의 Channel 개수이자, SPP Layer에 Input으로 Input feature map의 Channel 개수이다.
>
> 위의 구조를 예를 들면, M = 16 + 4 + 1 = 21이고, k = 256이다.
>
> 따라서, FC Layer의 Input vector size는 21 × 256 = 5,376 이다.

---

### ✅ 이미지 3

> 1. Selective Search를 이용하여 하나의 이미지당 약 2000개의 Region Proposal(RoI)을 생성한다
> 2. Image를 CNN에 통과시켜 output feature map을 얻는다
> 3. 각 RoI들을 Output Feature Map에 Projection 시켜 경계가 제한된 Feature Map을 얻는다
> 4. SPP Layer를 적용하여 얻은 fixed-length Feature Vector를 FC Layer에 Input으로 전달한다
> 5. Softmax & SVM으로 Classification(Category 분류)를 수행한다
> 6. Bounding Box Regression으로 Bounding Box의 크기를 조정하고 NMS(Non-maximum Suppression)을 사용하여 최종 bounding box를 선별한다

---

### ✅ 이미지 4

> SPP Net은 SPP라는 Pooling Layer가 여러개 결쳐있는 네트워크를 의미한다.
>
> Dense layer 뒷 부분은 기본적인 Object Detection Network들의 객체 클래스 분류를 위한 Softmax Layer와 SVM Classifier, 그리고 분류한 객체를 표시하는 Bounding Box Regression 구조로 RCNN 모델과 동일하다.

---

### ✅ 이미지 5

> **4. Spatial Pyramid Pooling**
>
> SPP는 특정 Pooling 과정을 통과하면 사이즈가 다른 Region Proposal들을 고정된 사이즈의 벡터들로 변환시키는 과정이다.
> 인풋 이미지 > 바운딩 박스 > CNNMODEL> FCLayer >이미지 분류
> 인풋 이미지 > CNNModel > 바운딩 박스 > FCLater> 이미지 분류
> 위에 보는 것처럼 SPP를 적용하지 않으면 이미지를 자르거나 찌그러뜨리는 단계가 있다. 단, SPP는 Convolution이 적용되는 CNN Model을 거쳐 나온 후 적용된다. 왜냐하면 SPP를 이용해서 동일한 크기 사이즈의 벡터로 결합을 하고 Flatten 시켜 FC Layer에 만들어야 하기 때문이다.

---

fast r-cnn 
오브젝트 디텍션의 메인 포인트는 해당 오브젝트의 위치를 바운딩 박스로 찾고, 바운딩박스 내 오브젝트를 판별하는 것이다

바운딩박스 리그레션(바운딩 박스가 어딨는지 좌표값을 예측하고 오브젝트에 핏 하도록 좌표를 조정)
classification(바운딩 박스 안에 있는 건 무엇인가?) 두개의 문제를 같이 해결하는 것이다

논문에서는 이를 각각 classify object proposals and refine their spatial locations라 한다


---

### ✅ 이미지 1

#### 1. Multi-Stage Pipeline

* Regional Proposal & Feature Extractor → Softmax & SVM → Bounding Box Regression이라는 3단계 Pipeline의 한계가 있다
* Pipeline의 각 단계가 독립적으로 최적화(fine-tune)되기 때문에 한 단계에서 발생한 작은 오류가 다음 단계로 전달될 수 있으며, 각 단계마다 오류가 누적되어 전체 결과의 정확도에 영향을 미칠 수 있다
  → 따라서 **Back Propagation이 불가능하다**

#### 2. Expensive in space and time

* 하나의 사진 안에 있는 각 Region Propsal마다 Object Detection을 별개로 수행해줘야 해서 매우 오랜 학습시간이 소모된다

#### 3. Slow Object Detection

* 각 test image 안에 있는 여러 object proposal에 대해 각각 Feature Extractor에 넣어줘야 한다 (2번이랑 비슷하다)

#### 4. Image Loss

* 여러 개의 서로 다른 size의 Region Proposal들을 Pretrain된 Feature Extractor가 요구하는 고정된 size로 통일시켜줘야 해서 region proposal을 자르거나 찌그러뜨려 image의 손실이 일어나게 된다

---

### ✅ 이미지 2

**SPPnet도 다음과 같은 두 개의 단점을 가진다.**

#### 1. Multi-Stage Pipeline

* Extracting Features, Fine-Tuning Networks w/ log loss, Training SVMs, Fitting Bounding Box Regressors의 Pipeline의 한계
  → 따라서 **여전히 Backpropagation이 불가능하다**

#### 2. Fixed Convolutional Layers

* SPPnet에서 사용된 Fine-Tuning algorithm은 SPP 이전에 있는 Convolutional Layer들을 업데이트할 수 없다
* 즉 네트워크의 초기 Layer들은 Pre-trained된 가중치를 유지하고 변경하지 않는데, 이는 **Deep Network의 정확도를 제한한다**

---

### ✅ 이미지 3

#### Fast R-CNN: Preview

**Fast R-CNN은 다음과 같은 4가지의 Advantage가 있다:**

* Higher detection quality (mAP) than R-CNN, SPPnet
* Training is single-stage & Multi-Task Loss 사용가능
* Training의 모든 Network Layer들을 업데이트 할 수 있다
* Disk Storage가 요구되지 않는다

**구조 설명:**

1. 모두 동일한 사이즈의 Region Proposal을 만들기 위해 SPP를 사용하지 않고 **ROI(Regions Of Interest) Pooling**을 사용한다
2. Object가 무엇인지 클래스를 분류할 때 SVM Classifier를 사용하지 않고 **Softmax Layer만을 사용한다**

---

### ✅ 이미지 4

**Fast RCNN도 RCNN과 똑같이 처음에 Selective Search를 통해 Region Proposal을 뽑아내긴 한다.**
하지만 R-CNN과 다르게 **뽑아낸 영역을 Crop하지 않고 그대로 가지고 있고**,
**전체 이미지를 CNN 모델에 집어 넣은 후,**
CNN으로부터 나온 **Feature Map에 RoI Projection을** 하는 방식이다.

→ 즉 input image 1장으로부터 CNN Model에 들어가는 이미지는 **2000장 → 1장**이 된다.

---

### ✅ 이미지 5

이 Feature Map에 Projection 한 RoI들을 **RoI Pooling** 하는 것이 **Fast R-CNN의 핵심**이다.
위 그림처럼 Projection시킨 RoI를 \*\*FCs(Fully Connected Layer)\*\*에 넣기 위해서는 같은 Size의 Feature map이 필요하다.

하지만 Selective Search를 통해 추려냈던 RoI 영역은 각각 다른 크기를 가지고 있고, 따라서 이 Resolution의 크기를 맞춰주기 위해 **RoI Pooling**을 수행한다.

사실 SPP 과정도 ROI Pooling 과정이라고 할 수 있다. ROI Pooling의 구체적인 정의는
**서로 크기가 다른 Region Proposal을 Fixed Size Vector로 만드는 것**을 의미한다.
여기서는 SPPnet과의 차이를 강조하기 위해 Fast RCNN에서는 ROI Pooling이라고 정의하였다.

즉, 크기가 다른 Feature Map의 Region마다 Stride를 다르게 Max Pooling을 진행하여 결과값을 맞추는 방법이다.

---

![alt text](image.png)

8x8의 인풋 피처맵에서  selective search로 뽑아냈던 5x7 짜리 
region proposal 부분이 있고
이를 2x2 로 만들어 주기 위해 stride(7/2,5/2)로 pooling sections를 정하고 
maxpooling 하여 2x2 output을 얻어낸ㄷ

아래는 이미지에서 추출한 전체 텍스트입니다:

---

Fast R-CNN에서 먼저 입력 이미지를 CNN에 통과시켜 Feature Map을 추출하고 이전에 미리 Selective Search로 만들어놨던 Region Proposal을 Feature Map에 Projection 시킨다.

따라서 그림은 Feature Map에 $h \times w$ 크기의 검은색 box가 projection된 Region Proposal이다.

**Projection된 Region Proposal:** $h \times w$

**고정된 크기의 Feature Vector:** $H \times W$

* $H, W$는 Layer Hyperparameter이다
* 임의의 RoI Size에 대해 independent하다

**Pooling Section(each grid):** $\frac{h}{H} \times \frac{w}{W}$

---
![alt text](image-1.png)

### Fast R-CNN

이때, Pre-Trained Network가 Fast R-CNN Network를 Initialize하면 다음의 3가지 **transformation**이 일어난다.

1. 마지막 max pooling을 첫 FC layer와 H, W가 호환되도록 세팅된 **RoI pooling layer로 변경**하였다
2. 마지막 FC layer와 softmax를 이전에 말한 **2개의 Layer인 K+1 softmax, Bbox regressor로 변경**한다
3. Image List와 RoI List **2가지를 Input으로 사용**한다

---

### Softmax

마지막으로 **Fixed Length Feature Vector**를 FCs(Fully Connected Layer)에 집어 넣은 후, 두 자식 layer인 output layer로 뻗어 나가 **Classification과 Bounding box Regression**을 진행 한다.

이는 R-CNN과 비슷하게 진행 하였지만 Fast R-CNN은 **Softmax를 사용하여 Classification**을 진행 하였다.

---

하나의 로스값을 이용해서 두가지 모델을 학습할수있는 방법을 만듬 
Classification, Bounding Box Regression  을 동히에 해결
multi task loss


---

$$
L(p, u, t^u, v) = L_{cls}(p, u) + \lambda [u \geq 1] L_{loc}(t^u, v)
$$

* **Classification**

* **Bounding box Regression**

* $p = (p_0, \dots, p_K)$: 예측된 Class Score (총 $K + 1$ categories, 0은 background)

* $u$: 실제 Class Score

* $t^u = (t_x^u, t_y^u, t_w^u, t_h^u)$: 예측된 tuple

* $v = (v_x, v_y, v_w, v_h)$: 실제 Bounding box 좌표 값

> $u = 0$ (background)일 때 0, 나머지일 땐 1

---

Classification과 Bbox Regression을 동시에 해결해서 **Multi-task loss**라고도 불린다.

> **Classification에서는 Cross Entropy를 사용**하고
> **Bbox Regression에서는 Smooth L1 함수**를 이용한다.

---

### Multi-Task Loss

Fast R-CNN은 두 가지 Output Layer로 나뉘게 된다:

* $K + 1$개의 Category에 대한 **Softmax를 이용한 Classification** (Discrete Probability Distribution)
* $K$개의 Category(Object Classes)에 대한 **Bounding Box Regression Offset**

---

이제, 이 **두 가지를 동시에 학습**하기 위해서 **Multi-Task Loss**를 사용한다.

* $u$: ground-truth class (one-hot encoding)
* $v$: ground-truth bounding box regression target
* $p$: predicted class score
* $t^u$: predicted tuple
* $[u \geq 1]$: $u$가 1 이상일 때 1의 값, 그 외에는 0 값을 가지는 함수(indicator)

  * 이를 이용해 background는 $u = 0$으로 Labeling 되어 loss 계산에서 무시가 가능하다

---

smooth l1 함수에서 x값에 따라 함수식이 달라진다 여기서 x는 오차(Error) 이다

절대값 x <1 일때 L2 Loss유형의

l1 못쓸때는 l2로스를 쓰겠다 smooth l1

https://cs231n.stanford.edu/
https://cs231n.stanford.edu/slides/2022/lecture_9_jiajun.pdf

실생활에서 객체들을 인식하는 방법
CNN(합성곱신경망)을 적용하여 만들어진 한장의 이미지를 주어졌을때 고양이인지를 판단하는 분류 방법 보다 더 고도호된 알고리즘
실생활에서 여러 객체들을 동시에 인식하고 라벨링하고 크기를 판단할 수 있음
일상 생활 및 산업현장에서 AI를 사용할때 필수적으로 적용
2019년 미국에서 자율 주행 중이었던 우버의 볼보 XC90이 밤에 무단 횡단하던 여성을 그대로 침

실생활에서 객체들을 인식하는 방법
CNN(합성곱신경망)을 적용하여 만들어진 한장의 이미지를 주어졌을때 고양이인지를 판단하는 분류 방법 보다 더 고도화된 알고리즘
실생활에서 여러 객체들을 동시에 인식하고 라벨링하고 크기를 판단할 수 있음

Box Proposal Method : Selective Search
박스로 오브젝트를 구분한 다음, 해당 객체에 색칠을 해서 정확도를 높이는 방법을 사용함!
객체인식 아이디어
오브젝트들을 박스로 감싸면서 분리하고, 박스안에 들어온 오브젝트에 대해서 분류를 같이 수행!
R-CNN
입력 이미지 -> 박스영역별 분류 -> 박스 영역별 CNN 특징 추출 및 그후 분류 작업을 거침  총 4단계
Fast-RCNN
모든 상자들에 대해서 특징들을 매번 계산할 필요 없이 가장 중요한 박스만 선정해서 계산을 진행 하는 방식을 제안 
Fast-RCNN
바운딩 박스 영역들에 대한 분류부분을 CNN 에 의한 분류과정에서 같이 포함을 하도록 처리함 (레이어를 통해서 나온 정보를 바탕으로)
YOLO- You Only Look Once
이미지에서 바운딩 박스를 찾는 방법을 사용하지 않음, 그리드(격자)로 만들어진 부분들에서 계산을 해서 바운딕 박스와 클래스피케이션을 동시에 수행
한번만 이미지를 바으면 거기서 객체 인식을 완료하여 YOLO라고 불림
YOLO의 대유행
초기 연구자 – 동료-동료-지인 – 지인 – 등을 거쳐 상관 없는 사람들도 객체 인식 방법이 빠르면 YOLO라고 이름을 붙임…
R-CNN의 경우..
나온지 너무 오래된 모델이어서 구하기가 어려움 (TF 1.0 으로 개발된 경우가 많음)
Faster R-CNN 이 더 빠른 속도로 제공해주고 있어서 해당 모델에 대한 분석 및 실습을 진행해볼 예정
COCO 데이터셋
COCO 데이터셋은 객체 탐지 (object detection), 세그먼테이션 (segmentation), 키포인트 탐지 (keypoint detection) 등의 컴퓨터 비전(computer vision) 분야의 task를 목적으로 만들어진 데이터셋
object detection 관련 논문을 읽어 보면, 논문에서 성능 평가 목적으로 많이 사용되는 데이터셋 중에서 COCO 2017이 가장 aksgdmsbus
 ▶ 학습(training) 데이터셋: 118,000장의 이미지
 ▶ 검증(validation) 데이터셋: 5,000장의 이미지
 ▶ 테스트(test) 데이터셋: 41,000장의 이미지
https://cocodataset.org/ 
